{"cells":[{"cell_type":"markdown","metadata":{"id":"FdtjTjySiUs_"},"source":["# AIFair360 Demo: Binary Classification with the German Credit Dataset\n","\n","The goal of this tutorial is to introduce the basic functionality of AI Fairness 360 to an interested developer who may not have a background in bias detection and mitigation.\n","\n","## Biases and Machine Learning \n","\n","A machine learning model makes predictions of an outcome for a particular instance. (Given an instance of a loan application, predict if the applicant will repay the loan.) The model makes these predictions based on a training dataset, where many other instances (other loan applications) and actual outcomes (whether they repaid) are provided. Thus, a machine learning algorithm will attempt to find patterns, or generalizations, in the training dataset to use when a prediction for a new instance is needed. (For example, one pattern it might discover is \"if a person has a salary > USD 40K and has outstanding debt < USD 5, they will repay the loan\".) \n","\n","However, there are scenarios in which these patterns may not be fair. Imagine a situation in which our algorithms predict a positive outcome for male candidates 80% of the time, but only 20% for females or vice versa. This is an example of bias in machine learning. It can be caused by different circumstances like underrepresentation in training data.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"en9xok1IiUtI"},"source":["**AIFair360** is designed to help address this problem with fairness metrics and bias mitigators. \n","\n","1.  **Fairness metrics - can be used to check for bias in machine learning workflows.**\n","\n","    The research literature identifies 3 different categories of fairness metrics:\n","\n","    1.  Statistical measures: These are metrics that rely on statistical definitions such as True Positives, False Positives, False Negative Rate...\n","\n","    2.  Similarity-based / Individual measures: Statistical definitions largely ignore all attributes of the classified subject except the sensitive attribute. This may hide unfairness. Similarity-based measures attempt to address such issues by not marginalizing insensitive attributes. \n","\n","    3.  Causal reasoning: These definitions assume a given causal graph in which nodes represent attributes and edges relationships between them. These relations are captured by structural equations that aim to build algorithms that ensure a tolerable level of fairness. \n","\n","    Most metrics that can be calculated using the class aif360.metrics fall under statistical measures. Before going into examples of such measures, it is worth mentioning that they depend on the definition of protected/sensitive attributes, and consequently (un)protected groups/classes.\n","    \n","    - Protected attributes are features that may not be used as the basis for decisions. Protected attributes could be chosen because of legal mandates or because of organizational values. Some common protected attributes include race, religion, national origin, gender, marital status, age, and socioeconomic status. \n","    - (Un)protected groups (also known as (un)privileged groups/classes) are the groups that are consequently formed by splitting the data points with regards to the protected attributes. For example, if \"gender\" is defined as protected attribute (and it only takes either value \"female\" or \"male\"), the protected group would be all the data points defined as \"male\" in terms of this attribute.    \n","    \n","    Furthermore, most of these metrics are calculated as differences or ratios of the following rates:   \n","\n","    -   **Selection rate** = fraction of predicted labels matching the positive outcome\n","\n","    -   **True positive rate** = fraction of positive cases correctly predicted to be in the positive class out of all actual positive cases\n","\n","    -   **False positive rate** = fraction of negative cases incorrectly predicted to be in the positive class out of all actual negative cases\n","\n","    -   **False negative rate** = fraction of positive cases incorrectly predicted to be in the negative class out of all actual positive cases\n","\n","    -   **True negative rate** = fraction of negative cases correctly predicted to be in the negative class out of all actual negative cases\n","\n","    The following are a few relevant examples of statistical measures calculated on the rates enumerated above:\n","\n","    -   **Demographic parity difference** = difference between largest and smallest group-level selection rate (0 means all groups have the same selection rate)\n","\n","        -   **Demographic parity ratio** = similar to above\n","\n","    -   **False positive/negative rate difference**\n","\n","    -   **Equalized odds difference** = quantifies the disparity in accuracy experienced by different demographics; the larger of the following: (1) false positive rate difference and (2) false negative rate difference\n","\n","        -   **Equalized odds ratio** = similar to above\n","\n","    A description of how AIFair360 approaches fairness assessment can be found here <https://aif360.mybluemix.net/resources#guidance>, while more metric examples are described here <https://aif360.readthedocs.io/en/latest/modules/metrics.html>.\n","\n","\n","2.  **Bias mitigators - can be used to overcome bias in the workflow to produce a more fair outcome.**\n","\n","    Bias can enter the system in 3 critical steps:\n","\n","    -   Training data: outcomes may be biased towards particular kinds of instances\n","        - Example: Imagine that historically people under the age of 30 years old were always refused a loan because of stereotypes around young people. The training data collected in such times would then have very few (or even none) samples for this age group with accepted loans. If we now consider that age shouldn’t play a role anymore in this decision (thus young people should receive as many loans as older people), the training dataset wouldn’t be representative of the ideal situation, and instead be biased. It is easy thus to imagine that a model trained on this dataset would also return biased outcomes. \n","\n","    -   Algorithm that creates the model: it may generate models that are weighted towards particular features in the input\n","        - Example: Regularization refers to techniques that are used to calibrate ML models in order to minimize the adjusted loss function and prevent overfitting or underfitting. Regularization will help select a midpoint between high bias and high variance. This ideal goal of generalization in terms of bias and variance is a low bias and a low variance which is near impossible or difficult to achieve. Hence, the need of the trade-off and consequently the possibility of introducing bias. \n","\n","    -   Test data: it has expectations on correct answers that may be biased\n","\n","        - Example: You can think about this in a similar way as for the training data.\n","\n","    These 3 points in the machine learning process represent points for testing and mitigating bias. In AIF360, we call these points pre-processing, in-processing, and post-processing. The toolkit provides methods that can be applied in each of these stages, these are listed below:\n","\n","    1.  **Preprocessing algorithms** transform the training data to mitigate possible unfairness. Preprocessing algorithms in AIFair360 follow the [sklearn.base.TransformerMixin](https://scikit-learn.org/stable/modules/generated/sklearn.base.TransformerMixin.html#sklearn.base.TransformerMixin) class, meaning that they can fit to the dataset and transform it.\n","\n","         - **Correlation Remover** = projects away correlations between sensitive and non-sensitive features in the dataset while details from the original data are retained as much as possible\n","\n","    2.  **In-processing algorithms** enable unfairness mitigation with respect to user-provided fairness constraints (currently only group-fairness constraints are supported, where group-fairness means that subjects from both protected and unprotected groups have equal probabilities of being assigned to the positive predictive class). \n","        - **Prejudice remover** = adds a discrimination-aware regularization term to the learning objective\n","\n","    3.  **Post-processing algorithms** modify results of previously trained classifiers to achieve the desired results on different groups.\n","\n","        - **Calibrated equality of odds** =  optimizes over calibrated classifier score outputs to find probabilities with which to change output labels with an equalized odds objective\n","\n","Note that these are just examples and the complete list of mitigation algorithms can be found at: <https://aif360.readthedocs.io/en/latest/modules/algorithms.html>. \n","\n","The rest of this notebook will guide you through a few examples on how to use these metrics and mitigation methods in practice."]},{"cell_type":"markdown","metadata":{"id":"MvuV548iiUtV"},"source":["## Contents\n","\n","1. [What is Covered](#What-is-Covered)\n","1. [Introduction: Import statements](#Introduction)\n","1. [The German Credit Dataset](#The-German-Credit-Dataset)\n","1. [Using a Fairness Unaware Model](#Using-a-Fairness-Unaware-Model)\n","1. [Preprocessing mitigation with Reweighing](#Preprocessing-Mitigation-using-Reweighing)\n","1. [Inprocessing mitigation with Prejudice Remover](#Inprocessing-Mitigation-using-Prejudice-Remover)\n","1. [Conclusion](#Conclusion)"]},{"cell_type":"markdown","metadata":{"id":"CnMydGnfiUtZ"},"source":["## What is Covered\n","\n","* **Domain:**\n","  * Finance (loan decisions). \n","\n","* **ML task:**\n","  * Binary classification.\n","\n","* **Fairness tasks:**\n","  * Assessment of unfairness using aif360 metrics.\n","  * Mitigation of unfairness using aif360 mitigation algorithms.\n","\n","* **Performance metrics:**\n","  * Accuracy.\n","  * Balanced accuracy.\n","  * Error Rate (difference).\n","\n","* **Fairness metrics:**\n","  * False-positive rate difference.\n","  * False-negative rate difference.\n","  * Statistical parity difference.\n","  * Averaged odds difference.\n","\n","* **Mitigation algorithms:**\n","\n","  * `aif360.algorithms.inprocessing.PrejudiceRemover`\n","  * `aif360.algorithms.preprocessing.Reweighing`"]},{"cell_type":"markdown","metadata":{"id":"76S_Mxb0iUtc"},"source":["## Introduction\n","\n","We consider a scenario where algorithmic tools are deployed to predict the likelihood that an applicant will default on a credit-card loan. For this, we will use the [German credit dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28German+Credit+Data%29), a dataset reflecting credit-card defaults in Germany, as a substitute dataset to replicate the desired workflow.\n","\n","We train a fairness-unaware algorithm on this dataset and show the model has a higher false-positive rate as well as a higher false-negative rate for the \"male\" group than for the \"female\" group. We then use aifair360 to mitigate this disparity using both the `Reweighting` and `Prejudice remover` algorithms."]},{"cell_type":"markdown","metadata":{"id":"clkd-MsCiUtf"},"source":["### The German Credit Dataset\n","This dataset is publicly available on the UCI Machine Learning Repository. It consists of 1000 loan applicants, with no missing values. There are 20 attributes\n"]},{"cell_type":"code","source":["!pip install aif360[all]"],"metadata":{"id":"SWYGIPh0mn0d"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zh6HCG3oiUth"},"outputs":[],"source":["# Load all necessary packages\n","import sys\n","sys.path.insert(1, \"../\")  \n","\n","import numpy as np\n","np.random.seed(0)\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import lightgbm as lgb\n","from sklearn.calibration import CalibratedClassifierCV\n","\n","from aif360.datasets import GermanDataset\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.algorithms.preprocessing import Reweighing\n","from aif360.algorithms.inprocessing import PrejudiceRemover\n","\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.pipeline import make_pipeline\n","from aif360.datasets import GermanDataset\n","from aif360.metrics import BinaryLabelDatasetMetric\n","from aif360.metrics import ClassificationMetric\n","\n","\n","from IPython.display import Markdown, display"]},{"cell_type":"markdown","metadata":{"id":"yVC441G0iUtm"},"source":["### Load dataset, specifying protected attribute, and split dataset into train and test\n","We load the initial dataset, setting the protected attribute to be sex.  We then splits the original dataset into training and testing datasets.  Although we will use only the training dataset in this tutorial, a normal workflow would also use a test dataset for assessing the efficacy (accuracy, fairness, etc.) during the development of a machine learning model.  Finally, we set two variables for the privileged (1) and unprivileged (0) values for the sex attribute.  These are key inputs for detecting and mitigating bias, which will be done later in the notebook.  "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g8UKjCluiUto"},"outputs":[],"source":["def preproc_and_load_data_german():\n","    \"\"\"\n","    Load and pre-process german credit dataset.\n","    Args: -\n","    Returns:\n","        GermanDataset: An instance of GermanDataset with required pre-processing.\n","    \"\"\"\n","    def custom_preprocessing(df):\n","        \"\"\" Custom pre-processing for German Credit Data\n","        \"\"\"\n","\n","        def group_credit_hist(x):\n","            if x in ['A30', 'A31', 'A32']:\n","                return 'None/Paid'\n","            elif x == 'A33':\n","                return 'Delay'\n","            elif x == 'A34':\n","                return 'Other'\n","            else:\n","                return 'NA'\n","\n","        def group_employ(x):\n","            if x == 'A71':\n","                return 'Unemployed'\n","            elif x in ['A72', 'A73']:\n","                return '1-4 years'\n","            elif x in ['A74', 'A75']:\n","                return '4+ years'\n","            else:\n","                return 'NA'\n","\n","        def group_savings(x):\n","            if x in ['A61', 'A62']:\n","                return '<500'\n","            elif x in ['A63', 'A64']:\n","                return '500+'\n","            elif x == 'A65':\n","                return 'Unknown/None'\n","            else:\n","                return 'NA'\n","\n","        def group_status(x):\n","            if x in ['A11', 'A12']:\n","                return '<200'\n","            elif x in ['A13']:\n","                return '200+'\n","            elif x == 'A14':\n","                return 'None'\n","            else:\n","                return 'NA'\n","        \n","        def group_personal_status(x):\n","            if x in ['A91']:\n","                return 'divorced/separated'\n","            elif x in ['A92']:\n","                return 'divorced/separated/married'\n","            elif x in ['A93', 'A95']:\n","                return 'single'\n","            elif x in ['A94']:\n","                return 'married/widowed'\n","            else:\n","                return 'NA'\n","\n","        status_map = {'A91': 1.0, 'A93': 1.0, 'A94': 1.0,\n","                    'A92': 0.0, 'A95': 0.0}\n","        \n","        df['sex'] = df['personal_status'].replace(status_map)\n","        \n","\n","        # group credit history, savings, and employment\n","        df['credit_history'] = df['credit_history'].apply(lambda x: group_credit_hist(x))\n","        df['savings'] = df['savings'].apply(lambda x: group_savings(x))\n","        df['employment'] = df['employment'].apply(lambda x: group_employ(x))\n","        #df['age'] = df['age'].apply(lambda x: np.float(x >= 26))\n","        df['status'] = df['status'].apply(lambda x: group_status(x))\n","        df['personal_status'] = df['personal_status'].apply(lambda x: group_personal_status(x))\n","        \n","        return df\n","\n","    # Feature partitions\n","    XD_features = ['number_of_credits', 'telephone',\n","                     'foreign_worker', 'people_liable_for', 'skill_level', 'credit_history',\\\n","                   'installment_plans', 'residence_since', 'property', 'other_debtors', \\\n","                   'purpose', 'savings', 'employment', 'sex', 'age', 'month']\n","    D_features = ['sex'] \n","    Y_features = ['credit']\n","    X_features = list(set(XD_features)-set(D_features))\n","    categorical_features = ['installment_plans', 'telephone',\n","                     'foreign_worker', 'skill_level', 'credit_history', 'property',\\\n","                            'other_debtors', 'purpose', 'savings', 'employment']\n","\n","    # privileged classes\n","    all_privileged_classes = {\"sex\": [1.0]}\n","\n","    # protected attribute maps\n","    all_protected_attribute_maps = {\"sex\": {1.0: 'Male', 0.0: 'Female'}}\n","\n","    return GermanDataset(\n","        label_name=Y_features[0],\n","        favorable_classes=[1],\n","        protected_attribute_names=D_features,\n","        privileged_classes=[all_privileged_classes[x] for x in D_features],\n","        instance_weights_name=None,\n","        categorical_features=categorical_features,\n","        features_to_keep=X_features+Y_features+D_features,\n","        features_to_drop=[],\n","        metadata={ 'label_maps': [{1.0: 'Good Credit', 2.0: 'Bad Credit'}],\n","                   'protected_attribute_maps': [all_protected_attribute_maps[x]\n","                                for x in D_features]},\n","        custom_preprocessing=custom_preprocessing)\n","\n","dataset_orig = preproc_and_load_data_german()\n","privileged_groups = [{'sex': 1}]\n","unprivileged_groups = [{'sex': 0}]"]},{"cell_type":"markdown","source":["#### Display database information"],"metadata":{"id":"HEeIN-oZQY7V"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"77ekNP1FiUtt","executionInfo":{"status":"ok","timestamp":1653634619830,"user_tz":-120,"elapsed":60,"user":{"displayName":"Pablo Biedma","userId":"00217242385302305130"}},"outputId":"f8d0499c-e07e-48c1-9fa7-eafe33b4f20c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of records:  1000\n","Number of features:  43\n","Names of features:  ['month', 'residence_since', 'age', 'number_of_credits', 'people_liable_for', 'sex', 'credit_history=Delay', 'credit_history=None/Paid', 'credit_history=Other', 'purpose=A40', 'purpose=A41', 'purpose=A410', 'purpose=A42', 'purpose=A43', 'purpose=A44', 'purpose=A45', 'purpose=A46', 'purpose=A48', 'purpose=A49', 'savings=500+', 'savings=<500', 'savings=Unknown/None', 'employment=1-4 years', 'employment=4+ years', 'employment=Unemployed', 'other_debtors=A101', 'other_debtors=A102', 'other_debtors=A103', 'property=A121', 'property=A122', 'property=A123', 'property=A124', 'installment_plans=A141', 'installment_plans=A142', 'installment_plans=A143', 'skill_level=A171', 'skill_level=A172', 'skill_level=A173', 'skill_level=A174', 'telephone=A191', 'telephone=A192', 'foreign_worker=A201', 'foreign_worker=A202']\n"]}],"source":["# Number of records:\n","print(\"Number of records: \",  dataset_orig.features.shape[0])\n","# Number of features:\n","print(\"Number of features: \",  dataset_orig.features.shape[1])\n","# Feature names:\n","print(\"Names of features: \",  dataset_orig.feature_names)"]},{"cell_type":"markdown","source":["#### Split into training, validation, and testing data."],"metadata":{"id":"2f3a4RhsQi8F"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"QvnhpLfviUtx"},"outputs":[],"source":["dataset_orig_train, dataset_orig_val, dataset_orig_test = \\\n","    dataset_orig.split([0.6, 0.8], shuffle=True, seed=1)"]},{"cell_type":"markdown","source":["#### Learning a logistic regression classifier."],"metadata":{"id":"HgCrJO50Vp2P"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"GvacS7ETiUtz"},"outputs":[],"source":["model = make_pipeline(StandardScaler(),\n","                      LogisticRegression(solver='liblinear', random_state=1))\n","fit_params = {'logisticregression__sample_weight': dataset_orig_train.instance_weights}\n","\n","lr_orig = model.fit(dataset_orig_train.features, dataset_orig_train.labels.ravel(), **fit_params)"]},{"cell_type":"markdown","metadata":{"id":"bcaXNrtsiUt1"},"source":["### Using a Fairness Unaware Model\n","Now that we've identified the protected attribute, and defined privileged and unprivileged values, we can use aif360 to detect bias in the dataset. The code below contains the metrics suitable for this task."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-QGzxj-giUt2"},"outputs":[],"source":["from collections import defaultdict\n","\n","def test(dataset, model, thresh_arr):\n","    try:\n","        dataset_pred = dataset.copy(deepcopy=True)\n","        pos_ind = np.where(model.classes_ == dataset.favorable_label)[0][0]\n","        dataset_pred.scores = model.predict_proba(dataset_pred.features)[:,pos_ind].reshape(-1,1)\n","    except AttributeError:\n","        y_val_pred_prob = model.predict(dataset).scores\n","        pos_ind = 0\n","    \n","    metric_arrs = defaultdict(list)\n","    for thresh in thresh_arr:\n","        fav_inds = dataset_pred.scores > thresh\n","        dataset_pred.labels[fav_inds] = dataset_pred.favorable_label\n","        dataset_pred.labels[~fav_inds] = dataset_pred.unfavorable_label\n","\n","        # Computation of various metrics:\n","        metric = ClassificationMetric(\n","                dataset, dataset_pred,\n","                unprivileged_groups=unprivileged_groups,\n","                privileged_groups=privileged_groups)\n","        metric_arrs['accuracy'].append(metric.accuracy())\n","        metric_arrs['balanced_accuracy'].append((metric.true_positive_rate()\n","                                     + metric.true_negative_rate()) / 2)\n","        metric_arrs['avg_odds_diff'].append(metric.average_odds_difference())\n","        metric_arrs['disp_imp'].append(metric.disparate_impact())\n","        metric_arrs['stat_par_diff'].append(metric.statistical_parity_difference())\n","        metric_arrs['false_negative_rate_difference'].append(metric.false_negative_rate_difference())\n","        metric_arrs['false_positive_rate_difference'].append(metric.false_positive_rate_difference())\n","        metric_arrs['statistical_parity_difference'].append(metric.statistical_parity_difference())\n","        metric_arrs['selection_rate'].append(metric.selection_rate())\n","        metric_arrs['Error_Rate'].append(metric.error_rate())\n","        metric_arrs['Error_Rate_difference'].append(metric.error_rate_difference())\n","    return metric_arrs\n","\n","thresh_arr = np.linspace(0.01, 0.99, 100)"]},{"cell_type":"code","source":["def describe_metrics(metrics, thresh_arr):\n","    best_ind = np.argmax(metrics['balanced_accuracy'])\n","    print(\"Selection rate: {:6.4f}\".format(metrics['selection_rate'][best_ind]))\n","    print(\"Statistical parity difference: {:6.4f}\".format(metrics['stat_par_diff'][best_ind]))\n","    print(\"False Positive Rate Difference: {:6.4f}\".format(metrics['false_positive_rate_difference'][best_ind]))\n","    print(\"False negative Rate Difference: {:6.4f}\".format(metrics['false_negative_rate_difference'][best_ind]))\n","    print(\"Averaged odds difference: {:6.4f}\".format(metrics['avg_odds_diff'][best_ind]))\n","    print(\"Balanced Accuracy: {:6.4f}\".format(metrics['balanced_accuracy'][best_ind]))\n","    print(\"Accuracy: {:6.4f}\".format(metrics['accuracy'][best_ind]))\n","    print(\"Error Rate: {:6.4f}\".format(metrics['Error_Rate'][best_ind]))\n","    print(\"Error Rate difference: {:6.4f}\".format(metrics['Error_Rate_difference'][best_ind]))"],"metadata":{"id":"zMi9K0OOWMd6"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eGCr1mo1iUt4"},"outputs":[],"source":["thresh_arr = np.linspace(0.01, 0.5, 50)\n","test_metrics = test(dataset=dataset_orig_test,\n","                   model=lr_orig,\n","                   thresh_arr=thresh_arr)\n","lr_orig_best_ind = np.argmax(test_metrics['balanced_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oql7td9tiUt7"},"outputs":[],"source":["lr_orig_metrics = test(dataset=dataset_orig_test,\n","                       model=lr_orig,\n","                       thresh_arr=[thresh_arr[lr_orig_best_ind]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KQnUp79-iUt8","executionInfo":{"status":"ok","timestamp":1653634619837,"user_tz":-120,"elapsed":48,"user":{"displayName":"Pablo Biedma","userId":"00217242385302305130"}},"outputId":"aa3b127b-87a3-4cfa-aaac-864cd5250ea9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selection rate: 0.8500\n","Statistical parity difference: -0.0865\n","False Positive Rate Difference: -0.1652\n","False negative Rate Difference: 0.0278\n","Averaged odds difference: -0.0965\n","Balanced Accuracy: 0.6106\n","Accuracy: 0.7250\n","Error Rate: 0.2750\n","Error Rate difference: -0.0012\n"]}],"source":["describe_metrics(test_metrics, thresh_arr)"]},{"cell_type":"markdown","metadata":{"id":"abNV0jdviUt9"},"source":["As the overall performance metric we use balanced accuracy, which is suited to classification problems with a large imbalance between positive and negative examples. For binary classifiers, this is the same as the 'area under ROC curve (AUC).\n","\n","As the fairness metric we use *averaged odds difference*, which quantifies the disparity in accuracy experienced by different demographics. Our goal is to assure that neither of the two groups has substantially larger false-positive rates or false-negative rates than the other group. The averaged odds difference can be computed as: average difference of false positive rate (false positives / negatives) and true positive rate (true positives / positives) between unprivileged and privileged groups.\n","\n","The table above shows a balanced accuracy of 0.61 (based on continuous predictions) and the overall balanced error rate of 0.28 (based on 0/1 predictions). Both of these are satisfactory in our application context. As a sanity check, we also show the Statistical parity difference, whose level (slightly above -0.05) is considered satisfactory in this context same goes for the Averaged odds difference."]},{"cell_type":"markdown","metadata":{"id":"6Jz3b9RoiUt-"},"source":["### Preprocessing Mitigation using Reweighing\n","\n","The previous step showed that the privileged group was getting more positive outcomes in the training dataset.   Since this is not desirable, we are going to try to mitigate this bias in the training dataset.  As stated above, this is called _pre-processing_ mitigation because it happens before the creation of the model.  \n","\n","AI Fairness 360 implements several pre-processing mitigation algorithms.  We will choose the Reweighing algorithm [1], which is implemented in the `Reweighing` class in the `aif360.algorithms.preprocessing` package.  This algorithm will transform the dataset to have more equity in positive outcomes on the protected attribute for the privileged and unprivileged groups.\n","\n","We then call the fit and transform methods to perform the transformation, producing a newly transformed training dataset (dataset_transf_train).\n","\n","In the cells below, we transform the data for reweighing, then train a LR model on it, validate it, and test it, with a description of the resulting metrings also found below.\n","\n","`[1] F. Kamiran and T. Calders,  \"Data Preprocessing Techniques for Classification without Discrimination,\" Knowledge and Information Systems, 2012.`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TU1KKzwPiUuA"},"outputs":[],"source":["RW = Reweighing(unprivileged_groups=unprivileged_groups,\n","                privileged_groups=privileged_groups)\n","dataset_transf_train = RW.fit_transform(dataset_orig_train.copy())"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aIRmfA9qiUuB"},"outputs":[],"source":["dataset = dataset_transf_train\n","model = make_pipeline(StandardScaler(),\n","                      LogisticRegression(solver='liblinear', random_state=1))\n","fit_params = {'logisticregression__sample_weight': dataset.instance_weights}\n","lr_transf = model.fit(dataset.features, dataset.labels.ravel(), **fit_params)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfL4PuN2iUuB"},"outputs":[],"source":["thresh_arr = np.linspace(0.01, 0.5, 50)\n","test_metrics = test(dataset=dataset_orig_test,\n","                   model=lr_transf,\n","                   thresh_arr=thresh_arr)\n","lr_transf_best_ind = np.argmax(test_metrics['balanced_accuracy'])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"inyQHpPMiUuC"},"outputs":[],"source":["lr_transf_metrics = test(dataset=dataset_orig_test,\n","                         model=lr_transf,\n","                         thresh_arr=[thresh_arr[lr_transf_best_ind]])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87qjotbSiUuD","executionInfo":{"status":"ok","timestamp":1653634620289,"user_tz":-120,"elapsed":28,"user":{"displayName":"Pablo Biedma","userId":"00217242385302305130"}},"outputId":"8c2dc429-8884-44ed-a8bd-2426d9ef7d09"},"outputs":[{"output_type":"stream","name":"stdout","text":["Selection rate: 0.8700\n","Statistical parity difference: 0.0014\n","False Positive Rate Difference: -0.0044\n","False negative Rate Difference: -0.0222\n","Averaged odds difference: 0.0089\n","Balanced Accuracy: 0.6021\n","Accuracy: 0.7250\n","Error Rate: 0.2750\n","Error Rate difference: 0.0222\n"]}],"source":["describe_metrics(lr_transf_metrics, [thresh_arr[lr_transf_best_ind]])"]},{"cell_type":"markdown","source":["The reweighting greatly reduced the disparity in performance across multiple fairness metrics including error rate and averaged odds difference."],"metadata":{"id":"2TW7mCLRTYZn"}},{"cell_type":"markdown","metadata":{"id":"UPq1ChTTiUuG"},"source":["### Conclusion\n","Now that we have a transformed dataset, we can check how effective it was in removing bias by using the same metric we used for the original training dataset in steps above.  "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":225},"id":"1IpZqfCliUuH","executionInfo":{"status":"ok","timestamp":1653634620290,"user_tz":-120,"elapsed":27,"user":{"displayName":"Pablo Biedma","userId":"00217242385302305130"}},"outputId":"5df237b4-51db-4de2-a460-a928a5530964"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                accuracy  balanced_accuracy  avg_odds_diff  disp_imp  \\\n","Bias Mitigator                                                         \n","Unaware            0.725           0.610648      -0.096512  0.901360   \n","Reweighing         0.725           0.602074       0.008865  1.001613   \n","\n","                stat_par_diff  false_negative_rate_difference  \\\n","Bias Mitigator                                                  \n","Unaware             -0.086489                        0.027835   \n","Reweighing           0.001403                       -0.022165   \n","\n","                false_positive_rate_difference  statistical_parity_difference  \\\n","Bias Mitigator                                                                  \n","Unaware                              -0.165188                      -0.086489   \n","Reweighing                           -0.004435                       0.001403   \n","\n","                selection_rate  Error_Rate  Error_Rate_difference  \n","Bias Mitigator                                                     \n","Unaware                   0.85       0.275              -0.001169  \n","Reweighing                0.87       0.275               0.022207  "],"text/html":["\n","  <div id=\"df-237d2627-0be2-410a-a027-97c7064e88d8\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>accuracy</th>\n","      <th>balanced_accuracy</th>\n","      <th>avg_odds_diff</th>\n","      <th>disp_imp</th>\n","      <th>stat_par_diff</th>\n","      <th>false_negative_rate_difference</th>\n","      <th>false_positive_rate_difference</th>\n","      <th>statistical_parity_difference</th>\n","      <th>selection_rate</th>\n","      <th>Error_Rate</th>\n","      <th>Error_Rate_difference</th>\n","    </tr>\n","    <tr>\n","      <th>Bias Mitigator</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Unaware</th>\n","      <td>0.725</td>\n","      <td>0.610648</td>\n","      <td>-0.096512</td>\n","      <td>0.901360</td>\n","      <td>-0.086489</td>\n","      <td>0.027835</td>\n","      <td>-0.165188</td>\n","      <td>-0.086489</td>\n","      <td>0.85</td>\n","      <td>0.275</td>\n","      <td>-0.001169</td>\n","    </tr>\n","    <tr>\n","      <th>Reweighing</th>\n","      <td>0.725</td>\n","      <td>0.602074</td>\n","      <td>0.008865</td>\n","      <td>1.001613</td>\n","      <td>0.001403</td>\n","      <td>-0.022165</td>\n","      <td>-0.004435</td>\n","      <td>0.001403</td>\n","      <td>0.87</td>\n","      <td>0.275</td>\n","      <td>0.022207</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-237d2627-0be2-410a-a027-97c7064e88d8')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-237d2627-0be2-410a-a027-97c7064e88d8 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-237d2627-0be2-410a-a027-97c7064e88d8');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":19}],"source":["import pandas as pd\n","pd.set_option('display.multi_sparse', False)\n","results = [lr_orig_metrics, lr_transf_metrics]\n","debias = pd.Series(['Unaware']+['Reweighing'], name='Bias Mitigator')\n","pd.concat([pd.DataFrame(metrics) for metrics in results], axis=0).set_index([debias])"]},{"cell_type":"markdown","metadata":{"id":"5IZnrqafiUuH"},"source":["In this notebook, we explored how a fairness-unaware gradient boosted trees model performed on the classification task in contrast to the reweighted model and the prejudice removed model. \n","\n","The reweighting greatly reduced the disparity in performance across multiple fairness metrics including error rate and averaged odds difference.\n","\n","After engaging with relevant stakeholders, the data scientist can deploy the model that balances the performance-fairness trade-off that meets the needs of the business."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"colab":{"name":"AIF360 Demo.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}